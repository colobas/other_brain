---
title: Discovering Symbolic Models from Deep Learning with Inductive Biases
roam_key: https://arxiv.org/pdf/2006.11287.pdf
---

# Tags

[deep-learning](20201104234007-deep_learning.md) [symbolic-learning](20201104234024-symbolic_learning.md) [dynamical-systems](20201104234038-dynamical_systems.md)

# Motivation: When is a model knowledge?

- Is all knowledge encapsulable in a symbolic model?
    - Interesting ideas by Stephen Wolfram on this: "everything is a program"
        - Global narrative vs "simply a chain of details"


- Chaos theory

# Methods

- Step 1: Use a GNN to learn the system dynamics
    - Induce strong [inductive-bias](20201104234244-inductive_bias.md)
    - One vertex per particle in the system
        - Properties of the vertices are the properties of the particle (e.g. position, mass, charge&#x2026;)
    - All-to-all edges (in the physics example)
    - Assume interactions operate additively
        - Output of each node is a function of the **sum** of the edge messages

- Step 2: Extract equations from the GNN via symbolic regression
    - L1 regularization on the GNN to induce sparsity of edge messages
    - Use components with largest variation
